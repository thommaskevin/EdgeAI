# EdgeAI

This repository contains experiments and implementations focused on **Edge AI** ‚Äî running artificial intelligence models on resource-constrained devices. Each folder represents a different model, method, or approach optimized for edge deployment.

## üìÅ Directory Structure

| Folder            | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| `01_Llama`        | Implementations based on the **LLaMA** model family for on-device inference. |
| `02_LLaVA`        | Integration with **LLaVA** (Large Language and Vision Assistant) for multimodal tasks. |
| `03_Gemma2`       | Work with **Gemma 2**, a lightweight model by Google, adapted for edge use.  |
| `04_DeepSeek_R1`  | Experiments with **DeepSeek**, a Mistral-based model for efficient inference. |
| `05_Mistral_AI`   | Tests using the **Mistral AI** model, focused on lightweight performance.     |
| `06_RAG`          | **Retrieval-Augmented Generation (RAG)** implementations tailored for edge environments. |
| `07_ADD`          | Supporting tools, equations, and utility scripts for edge model evaluation.  |
| `08_LoRa`         | Experiments with **LoRA** (Low-Rank Adaptation) for lightweight fine-tuning. |

## üìå Purpose

The goal of this repository is to explore the deployment of advanced AI models on edge devices, emphasizing:

- Reduced computational complexity  
- Compression and quantization techniques  
- Lightweight fine-tuning (e.g., LoRA)  
- Integration with embedded systems and sensors  
- On-device RAG (Retrieval-Augmented Generation)

## üöß In Progress

This repository is a work in progress and subject to updates.  
Feel free to contribute, suggest improvements, or report issues!
